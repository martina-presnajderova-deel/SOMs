{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "\n",
    "#text information for the most significant banks\n",
    "#codes used within the reuters articles for the institutions\n",
    "banks_reuters = {\"Bank of New York Mellon\": ['BNY Mellon', 'Bank of New York Mellon', 'BK.N'], \"Bank of America\":['BAC.N', 'Bank of America', 'Bank of America\\'s'], \"BB_T Corporation\":['BB&T Corp', 'BB&T'], \"Capital One\":['COF.N', 'Capital One'], \"Citigroup\":['Citigroup', 'CIT2.L'], \"Fifth Third Bancorp\":['Fifth Third', 'FITB.OQ'], \"Goldman Sachs\":['Goldman', 'GS.N'], \"JP Morgan Chase\":['JP Morgan'], \"KeyCorp\":['KEY.N', 'KeyCorp'], \"Morgan Stanley\":['MS.N', 'Morgan Stanley'], \"PNC, State Street\":['PNC'], \"Sun Trust\":['Suntrust', 'SunTrust', 'STI.N'],\"Regions Financial\":['Regions', 'Regions Financial', 'RF.N'], \"U.S. Bancorp\":['Bancorp', 'USB.N'], \"Wells Fargo\":['Wells Fargo', 'WFC.N']}\n",
    "       \n",
    "def sentiment(inp):\n",
    "    bank = tag[0]\n",
    "    score = 0\n",
    "    r = requests.get(inp)\n",
    "    b = BeautifulSoup(r.content,'html.parser')\n",
    "    #get list of article links for each archive\n",
    "    links = b.select('div > div > div > div > div > div > a')\n",
    "    #open each link, extract article, extract company tag, run sentiment analysis, output score\n",
    "    sent_score = np.empty([1, 1], dtype=float)\n",
    "    for link in links:\n",
    "        try:\n",
    "            r1 = requests.get(link.get('href'))\n",
    "            if (r1.status_code==200): \n",
    "                b1 = BeautifulSoup(r1.text)\n",
    "                tags = b1.findAll(attrs={\"name\": \"news_keywords\"})[0]['content']\n",
    "                if [item for item in tag if item in tags] != []:\n",
    "                    #look for bank in bank tag dictionary\n",
    "                    desc = b1.findAll(attrs={\"name\": \"description\"})\n",
    "                    #sentiment\n",
    "                    sid = SIA()\n",
    "                    ss = sid.polarity_scores(desc[0]['content'].encode('utf-8'))\n",
    "                    ss = ss['compound'] #The compound score=sum of all of the lexicon ratings standardised to range between -1 and 1\n",
    "                    sent_score = np.append(sent_score, ss)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    score = int(np.nansum(sent_score) / float(len(sent_score)))\n",
    "    return(score)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map with Gaussian Neighbourhood function\n",
    "    and linearly decreasing learning rate.\n",
    "    \"\"\"\n",
    " \n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all necessary components of the TensorFlow\n",
    "        Graph.\n",
    " \n",
    "        m X n are the dimensions of the SOM. 'n_iterations' should\n",
    "        be an integer denoting the number of iterations undergone\n",
    "        while training.\n",
    "        'dim' is the dimensionality of the training inputs.\n",
    "        'alpha' is a number denoting the initial time(iteration no)-based\n",
    "        learning rate. Default value is 0.3\n",
    "        'sigma' is the the initial neighbourhood value, denoting\n",
    "        the radius of influence of the BMU while training. By default, its\n",
    "        taken to be half of max(m, n).\n",
    "        \"\"\"\n",
    " \n",
    "        #Assign required variables first\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        ##INITIALIZE GRAPH\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        ##POPULATE GRAPH WITH NECESSARY COMPONENTS\n",
    "        with self._graph.as_default():\n",
    " \n",
    "            ##VARIABLES AND CONSTANT OPS FOR DATA STORAGE\n",
    " \n",
    "            #Randomly initialized weightage vectors for all neurons,\n",
    "            #stored together as a matrix Variable of size [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Matrix of size [m*n, 2] for SOM grid locations\n",
    "            #of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            ##PLACEHOLDERS FOR TRAINING INPUTS\n",
    "            #We need to assign them as attributes to self, since they\n",
    "            #will be fed in during training\n",
    " \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            ##CONSTRUCT TRAINING OP PIECE BY PIECE\n",
    "            #Only the final, 'root' training op needs to be assigned as\n",
    "            #an attribute to self, since all the rest will be executed\n",
    "            #automatically during training\n",
    " \n",
    "            #To compute the Best Matching Unit given a vector\n",
    "            #Basically calculates the Euclidean distance between every\n",
    "            #neuron's weightage vector and the input, and returns the\n",
    "            #index of the neuron which gives the least value\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #This will extract the location of the BMU based on the BMU's\n",
    "            #index\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #To compute the alpha and sigma values based on iteration\n",
    "            #number\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            #Construct the op that will generate a vector with learning\n",
    "            #rates for all neurons, based on iteration number and location\n",
    "            #wrt BMU.\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Finally, the op that will use learning_rate_op to update\n",
    "            #the weightage vectors of all neurons based on a particular\n",
    "            #input\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            ##INITIALIZE SESSION\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            ##INITIALIZE VARIABLES\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        #Nested iterations over both dimensions\n",
    "        #to generate all 2-D locations in the map\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Current weightage vectors for all neurons(initially random) are\n",
    "        taken as starting conditions for training.\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to the relevant neuron in the SOM\n",
    "        grid.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Returns a list of 1-D NumPy arrays containing (row, column)\n",
    "        info for each input vector(in the same order), corresponding\n",
    "        to mapped neuron.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "require(caret)\n",
    "require(xtable)\n",
    "require(MASS)\n",
    "require(kohonen)\n",
    "\n",
    "\n",
    "##################\n",
    "#data\n",
    "##################\n",
    "setwd(\"~/Desktop/Diplomka/SystEvents\")\n",
    "count <- read.csv(file=\"country_factors.csv\", header=TRUE, sep=\",\")\n",
    "count$ShortIntR = as.numeric(count$ShortIntR)\n",
    "\n",
    "#outliers\n",
    "measures0 = c('Inflat', 'QGDP', 'GenGovDeficittoGDP', 'HousePriceIndex', 'CAbalancetoGDP', 'CredGovGDP', 'CredPrivateAllGDP', 'CredPrivateBanksGDP', 'CreditPrivateGapGDP', 'ShortIntR', 'LongIntR', 'CLIFS_cat', 'Name', 'location', 'Quarter', 'year', 'CLIFS')\n",
    "country = count[,measures0]\n",
    "for(i in 1:12) boxplot(country[,i], main=colnames(country)[i])\n",
    "#boxplot(count[,1])\n",
    "#boxplot(country[,1])\n",
    "#hist(country[,2])\n",
    "for (i in c(9,8,1:3)){\n",
    "  x<-quantile(country[,i],c(0.01,0.98))\n",
    "  country = country[country[,i] >=x[1] & country[,i]<=x[2],]\n",
    "}\n",
    "#Define FSI categories\n",
    "for (i in 1:dim(country)[1]){\n",
    "  #calculate inverse quantile\n",
    "  quantInv = function(distr, value) ecdf(distr)(value)\n",
    "  x = country[which(country[,'location']==as.character(country[i,'location'])),'CLIFS']\n",
    "  country[i,'CLIFS_cat'] = quantInv(x, country[i,'CLIFS'])\n",
    "}\n",
    "#transform into categories\n",
    "for (i in 1:dim(country)[1]){\n",
    "  if(country[i,\"CLIFS_cat\"]>0.75 ) country[i,\"CLIFS_cat\"] = 3\n",
    "  if(country[i,\"CLIFS_cat\"]>0.50 & country[i,\"CLIFS_cat\"]<=0.75) country[i,\"CLIFS_cat\"]=2\n",
    "  if(country[i,\"CLIFS_cat\"]>0.25 & country[i,\"CLIFS_cat\"]<=0.50) country[i,\"CLIFS_cat\"]=1\n",
    "  if(country[i,\"CLIFS_cat\"]<=0.25) country[i,\"CLIFS_cat\"] = 0\n",
    "}\n",
    "\n",
    "table(country[,'CLIFS_cat'])\n",
    "country[,'CLIFS_log'] = as.numeric(country[,'CLIFS_cat']>2)\n",
    "\n",
    "#train/test\n",
    "measures1 = c('Inflat', 'QGDP', 'GenGovDeficittoGDP', 'HousePriceIndex', 'CAbalancetoGDP', 'CredGovGDP', 'CredPrivateAllGDP', 'CredPrivateBanksGDP', 'CreditPrivateGapGDP', 'ShortIntR', 'LongIntR')\n",
    "risk_category = c('CLIFS_cat')\n",
    "bin_risk_category = c('CLIFS_log')\n",
    "\n",
    "training <- sample(nrow(country), round(dim(country)[1]/100*90,0))\n",
    "Xtrain_raw <- country[training, measures1]\n",
    "Xtest_raw = country[-training, measures1]\n",
    "Ytrain0 = as.factor(country[training, bin_risk_category])\n",
    "Ytest0 = as.factor(country[-training, bin_risk_category])\n",
    "Ytrain = as.factor(country[training, risk_category])\n",
    "Ytest = as.factor(country[-training, risk_category])\n",
    "\n",
    "\n",
    "#log regression\n",
    "# Logistics Regression\n",
    "glm.fit <- glm(Ytrain0 ~ ., data = Xtrain_raw, family = binomial)\n",
    "glm.probs <- predict(glm.fit,type = \"response\")\n",
    "glm.pred1 <- ifelse(glm.probs > 0.5, 1, 0)\n",
    "table(glm.pred1,Ytrain0)\n",
    "mean(glm.pred1 == Ytrain0)\n",
    "\n",
    "glm.probs <- predict(glm.fit, \n",
    "                     newdata = Xtest_raw, \n",
    "                     type = \"response\")\n",
    "glm.pred <- ifelse(glm.probs > 0.5, 1, 0)\n",
    "table(glm.pred,Ytest0)\n",
    "mean(glm.pred == Ytest0)\n",
    "\n",
    "#precision, accuracy, F\n",
    "result1 <- confusionMatrix(as.factor(glm.pred1),Ytrain0)\n",
    "result <- confusionMatrix(as.factor(glm.pred),Ytest0)\n",
    "\n",
    "  \n",
    "## fit ordered logit model and store results 'm'\n",
    "lr_ord <- polr(Ytrain ~ ., data = Xtrain_raw, Hess=TRUE)\n",
    "summary(lr_ord)\n",
    "(ctable <- coef(summary(lr_ord)))\n",
    "## calculate and store p values\n",
    "p <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n",
    "## combined table\n",
    "(ctable <- cbind(ctable, \"p value\" = p))\n",
    "## odds ratios\n",
    "exp(coef(lr_ord))\n",
    "\n",
    "#Scale\n",
    "Xtrain <- scale(Xtrain_raw)\n",
    "Xtest = scale(Xtest_raw, center = attr(Xtrain, \"scaled:center\"), scale = attr(Xtrain, \"scaled:scale\"))\n",
    "\n",
    "trainingdata <- list(measurements = Xtrain,risk_category = Ytrain)\n",
    "testdata <- list(measurements = Xtest, risk_category = Ytest)\n",
    "\n",
    "colors <- function(n, alpha = 1) {rev(heat.colors(n, alpha))}\n",
    "colors0 <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}\n",
    "colors1 <- function(n, alpha = 1) {rev(terrain.colors(n, alpha))}\n",
    "colour1 <- tricolor(som.risk$grid)\n",
    "colour2 <- tricolor(som.risk$grid, phi = c(pi/6, 0, -pi/6))\n",
    "colour3 <- tricolor(som.risk$grid, phi = c(pi/6, 0, -pi/6), offset = .5)\n",
    "\n",
    "\n",
    "## ################################################################\n",
    "## Situation 0: obtain expected values for training data (all layers,\n",
    "## also if not used in training) on the basis of the position in the map\n",
    "# som.risk <- supersom(trainingdata, grid = mygrid, rlen=1000)\n",
    "# som.prediction <- predict(som.risk)\n",
    "\n",
    "## ###############################################################\n",
    "## Situation 3: predictions for layers not present in the original\n",
    "## data. Training data need to be provided for those layers.\n",
    "#options(device = \"RStudioGD\")\n",
    "mygrid = somgrid(6, 9, \"hexagonal\")\n",
    "som.risk <- supersom(Xtrain, \n",
    "                     grid = mygrid, \n",
    "                     rlen=10,\n",
    "                     alpha=c(0.05,0.01),\n",
    "                     keep.data = TRUE)\n",
    "\n",
    "\n",
    "# plot(som.risk, type=\"counts\", shape = \"straight\", palette.name = colors1, main = \"How many samples are mapped to each node?\")\n",
    "# #Color the crises element: 1(low)-4(high)\n",
    "# var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "# names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "# plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Financial Stress Composite Indicator',shape='straight', palette.name=colors)\n",
    "\n",
    "\n",
    "#som.risk1 <- supersom(Xtrain, grid = mygrid, rlen=100)\n",
    "som.prediction <- predict(som.risk, newdata = testdata,\n",
    "                          trainingdata = trainingdata)\n",
    "table(country[-training, risk_category], som.prediction$predictions[[\"risk_category\"]])\n",
    "#precision, accuracy, F\n",
    "result <- confusionMatrix(as.factor(country[-training, risk_category]), as.factor(som.prediction$predictions[[\"risk_category\"]]))\n",
    "#type = c( \"changes\",\"counts\",\"codes\",\"dist.neighbours\", \"mapping\", \"property\", \"quality\")\n",
    "\n",
    "#Iteration\n",
    "plot(som.risk, type=\"changes\")\n",
    "par(mar=c(5.1,4.1,4.1,2.1))\n",
    "\n",
    "#Count\n",
    "#The Kohonen packages allows us to visualise the count of how many samples are mapped to each node on the map. This metric can be used as a measure of map quality – ideally the sample distribution is relatively uniform. Large values in some map areas suggests that a larger map would be benificial. Empty nodes indicate that your map size is too big for the number of samples. Aim for at least 5-10 samples per node when choosing map size. \n",
    "par(mfrow = c(1,1))\n",
    "plot(som.risk, type=\"counts\", shape = \"straight\", palette.name = colors1, main = \"How many samples are mapped to each node?\")\n",
    "plot(som.risk, type=\"mapping\", shape = \"straight\", main = \"How many samples are mapped to each node?\")\n",
    "\n",
    "\n",
    "#Neighbour Distance\n",
    "#the “U-Matrix” - visualisation of the distance between each node and its neighbours.Areas of low neighbour distance indicate groups of nodes that are similar. Areas with large distances indicate the nodes are much more dissimilar – and indicate natural boundaries between node clusters. The U-Matrix can be used to identify clusters within the SOM map. \n",
    "plot(som.risk, type=\"dist.neighbours\", palette.name = colors, shape='straight', main='The Unified Distance Matrix')\n",
    "\n",
    "#Because of low dimension - we can use Codes / Weight vectors\n",
    "#The node weight vectors, or “codes”, are made up of normalised values of the original variables used to generate the SOM. Each node’s weight vector is representative / similar of the samples mapped to that node. By visualising the weight vectors across the map, we can see patterns in the distribution of samples and variables. The default visualisation of the weight vectors is a “fan diagram”, where individual fan representations of the magnitude of each variable in the weight vector is shown for each node. Other represenations are available, see the kohonen plot documentation for details. \n",
    "plot(som.risk, type=\"codes\", shape='straight', main='Individual Factors')\n",
    "\n",
    "nam = c('Inflation','GDP', 'House Price Index', 'Quarterly Goverment Debt', 'Current Account Balance', 'Credit from Banks','Credit from All', 'Interest Rate - short', 'Interets Rate - long')\n",
    "\n",
    "\n",
    "#Heatmap\n",
    "#Each node coloured using average value of all linked datapoints\n",
    "#A SOM heatmap allows the visualisation of the distribution of a single variable across the map. Typically, a SOM investigative process involves the creation of multiple heatmaps, and then the comparison of these heatmaps to identify interesting areas on the map. It is important to remember that the individual sample positions do not move from one visualisation to another, the map is simply coloured by different variables.\n",
    "\n",
    "#variables \n",
    "#- shows distribution of variables across map\n",
    "#useful when number of parameters<5\n",
    "par(mfrow = c(1,1))\n",
    "\n",
    "\n",
    "nam = c('Inflation','GDP Change', \n",
    "        'Goverment Debt to GDP',\n",
    "        'House Price Index',\n",
    "        'Current Account Balance to GDP ', \n",
    "        'Credit to Goverment to GDP',\n",
    "        'Credit from All Sectors to GDP',\n",
    "        'Credit from Banks to GDP',\n",
    "        'Credit to GDP Gap',\n",
    "        'Interest Rate - 3M', \n",
    "        'Interets Rate - 10Y')\n",
    "\n",
    "for (i in 1:11) plot(som.risk, type = \"property\", property = som.risk$codes[[1]][,i], main=colnames(som.risk$data[[1]])[i], palette.name=colors)\n",
    "\n",
    "#Heatmap\n",
    "\n",
    "#A: Examine Heatmaps\n",
    "par(mfrow=c(1,1))\n",
    "par(mfrow = c(2, 2),     # 2x2 layout\n",
    "    oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin\n",
    "    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots\n",
    "    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row\n",
    "    xpd = NA) \n",
    "\n",
    "\n",
    "for (i in 1:11){\n",
    "  var <- i #define the variable to plot\n",
    "  # Plotting unscaled variables when there are empty nodes in the SOM\n",
    "  var_unscaled = aggregate(as.numeric(Xtrain_raw[[var]]), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "  names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "  # Add in NA values for non-assigned nodes - first find missing nodes:\n",
    "  missingNodes = which(!(seq(1,dim(som.risk$codes[[1]])[1]) %in% var_unscaled$Node))\n",
    "  # Add them to the unscaled variable data frame\n",
    "  var_unscaled = rbind(var_unscaled, data.frame(Node=missingNodes, Value=NA))\n",
    "  # order the resulting data frame\n",
    "  var_unscaled = var_unscaled[order(var_unscaled$Node),]\n",
    "  # Now create the heat map only using the \"Value\" which is in the correct order.\n",
    "  plot(som.risk, type = \"property\", property=var_unscaled$Value, main=nam[var], palette.name=colors,shape = \"straight\")\n",
    "}\n",
    "\n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Financial Stress Indicator',shape='straight', palette.name=colors)\n",
    "\n",
    "\n",
    "####Map country to SOM\n",
    "par(mfrow = c(2, 2),     # 2x2 layout\n",
    "    oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin\n",
    "    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots\n",
    "    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row\n",
    "    xpd = NA) \n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Euro Area Over Time',shape='straight', palette.name=colors)\n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Countries 2003',shape='straight', palette.name=colors)\n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Countries 2008',shape='straight', palette.name=colors)\n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='Countries 2012',shape='straight', palette.name=colors)\n",
    "\n",
    "mygrid = somgrid(6, 9, \"hexagonal\")\n",
    "som.risk <- supersom(Xtrain, \n",
    "                     grid = mygrid, \n",
    "                     rlen=10,\n",
    "                     alpha=c(0.05,0.01),\n",
    "                     keep.data = TRUE)\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "#Map \n",
    "#EA19\n",
    "head(country)\n",
    "country_sc = country\n",
    "country_sc[,measures1] = scale(country_sc[,measures1], center = attr(Xtrain, \"scaled:center\"), scale = attr(Xtrain, \"scaled:scale\"))\n",
    "filter = country_sc[which(country_sc['location']=='EA19' & country_sc['Quarter']=='Q1'),]\n",
    "\n",
    "xx = map(som.risk, as.matrix(filter[,measures1]))\n",
    "yy = som\n",
    "yy$data[[1]] = as.matrix(filter[,measures1])\n",
    "yy$unit.classif = xx$unit.classif\n",
    "yy$distances = xx$distances\n",
    "\n",
    "plot(yy, type=\"mapping\",\n",
    "     labels = filter[,'year'],\n",
    "     main = \"mapping plot\",shape='straight')\n",
    "\n",
    "\n",
    "#Map \n",
    "#2007Q3\n",
    "head(country)\n",
    "filter = country_sc[which(country_sc['year']==2006 & country_sc['Quarter']=='Q1'),]\n",
    "xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "yy = som.risk\n",
    "yy$data[[1]] = as.matrix(filter[,measures1])\n",
    "yy$unit.classif = xx$unit.classif\n",
    "yy$distances = xx$distances\n",
    "\n",
    "plot(som.risk, type=\"mapping\",\n",
    "     labels = filter[,'location'],\n",
    "     shape='straight',main='2006')\n",
    "\n",
    "#all\n",
    "for (i in (2006:2006)){\n",
    "  for (j in c('Q1', 'Q2', 'Q3', 'Q4')){\n",
    "    filter = country_sc[which(country_sc['year']==i & country_sc['Quarter']==j),]\n",
    "    xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "    yy = som.risk\n",
    "    yy$data[[1]] = as.matrix(filter[,measures1])\n",
    "    yy$unit.classif = xx$unit.classif\n",
    "    yy$distances = xx$distances\n",
    "    plot(som.risk, type=\"mapping\",\n",
    "         labels = filter[,'location'],\n",
    "         shape='straight',main=paste(i, j, sep=\"-\"))\n",
    "  }\n",
    "}\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#A: Examine Clusters\n",
    "#Clustering can be performed on the SOM nodes to isolate groups of samples with similar metrics. Manual identification of clusters is completed by exploring the heatmaps for a number of variables and drawing up a “story” about the different areas on the map. \n",
    "#An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm and examing for an “elbow-point” in the plot of “within cluster sum of squares”.  The Kohonen package documentation shows how a map can be clustered using hierachical clustering. The results of the clustering can be visualised using the SOM plot function again.\n",
    "#Hierarchical Clustering with Connectivity Constrains\n",
    "#In order to implement connectivity constrains we have to factor in the distance of each cluster to another cluster on the SOM map - kohonen::unit.distances takes a som grid and returns a distance matrix reflecting the positional distance among clusters on the som grid\n",
    "\n",
    "#Determining the optimal number of clusters\n",
    "#An estimate of the number of clusters that would be suitable can be ascertained using a kmeans algorithm and examing for an “elbow-point” in the plot of “within cluster sum of squares”. \n",
    "mydata <- som.risk$codes\n",
    "wss <- (nrow(mydata[[1]])-1)*sum(apply(mydata[[1]],2,var))\n",
    "for (i in 1:11) {\n",
    "  wss[i] <- sum(kmeans(mydata[[1]], centers=i)$withinss)\n",
    "}\n",
    "plot(wss)\n",
    "## use hierarchical clustering to cluster the codebook vectors\n",
    "som_cluster <- cutree(hclust(dist(som.risk$codes[[1]])), 6)\n",
    "# plot these results:\n",
    "# Colour palette definition\n",
    "pretty_palette <- c(\"#1f77b4\", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')\n",
    "plot(som.risk, type=\"mapping\", bgcol = pretty_palette[som_cluster],main = \"Clusters\") \n",
    "add.cluster.boundaries(som.risk, som_cluster)\n",
    "\n",
    "summary(som.risk)\n",
    "\n",
    "par(mfrow = c(1,1))\n",
    "similarities <- plot(som.risk, type=\"quality\", palette.name = terrain.colors)\n",
    "\n",
    "\n",
    "## add background colors to units according to their predicted class labels\n",
    "xyfpredictions <- classmat2classvec(getCodes(kohmap, 2))\n",
    "bgcols <- c(\"gray\", \"pink\", \"lightgreen\")\n",
    "plot(kohmap, type=\"mapping\", col = as.integer(vintages),\n",
    "     pchs = as.integer(vintages), bgcol = bgcols[as.integer(xyfpredictions)],\n",
    "     main = \"another mapping plot\", shape = \"straight\", border = NA)\n",
    "\n",
    "## Show 'component planes'\n",
    "sommap <- som(scale(wines), grid = somgrid(6, 4, \"hexagonal\"))\n",
    "plot(sommap, type = \"property\", property = getCodes(sommap, 1)[,1],\n",
    "     main = colnames(getCodes(sommap, 1))[1])\n",
    "\n",
    "## Show the U matrix\n",
    "Umat <- plot(som.risk, type=\"dist.neighbours\", main = \"SOM neighbour distances\")\n",
    "## use hierarchical clustering to cluster the codebook vectors\n",
    "som.hc <- cutree(hclust(object.distances(som.risk, \"codes\")), 5)\n",
    "add.cluster.boundaries(som.risk, som.hc)\n",
    "\n",
    "\n",
    "\n",
    "#WEIGHT VECTORS\n",
    "#each cell displays its representative weight vector\n",
    "count.som1 <- som(scale(country[measures1]), grid = somgrid(10, 10, \"rectangular\"),rlen = 4000)\n",
    "plot(count.som1)\n",
    "\n",
    "#HEATMAP\n",
    "#identify cells on the map by assigning each input to the cell with representative vector closest to that item’s stat line. The “count” type SOM does exactly this, and creates a heatmap based on the number of items assigned to each cell. \n",
    "# reverse color ramp so that red represents the most frequent\n",
    "par(mfrow = c(1, 1))\n",
    "\n",
    "colors <- function(n, alpha = 1) {\n",
    "  rev(heat.colors(n, alpha))\n",
    "}\n",
    "\n",
    "plot(count.som1, type = \"counts\", palette.name = colors, heatkey = TRUE)\n",
    "\n",
    "#HEATMAP 2.0\n",
    "plot(count.som1, type = \"mapping\", pchs = 20, main = \"Mapping Type SOM\")\n",
    "plot(count.som1, main = \"Default SOM Plot\")\n",
    "\n",
    "\n",
    "#DISTANCE\n",
    "#The cells are colored depending on the overall distance to their nearest neighbors, which allows us to visualize how far apart different features are in the higher dimensional space.\n",
    "plot(count.som1, type = \"dist.neighbours\", palette.name = terrain.colors)\n",
    "\n",
    "#Supervised SOMs\n",
    "#classification of items by their FSI\n",
    "#Randomly divide data into training and testing sets\n",
    "training <- sample(nrow(country), round(dim(country)[1]/100*80,0))\n",
    "train <- scale(country[training, measures1])\n",
    "test <- scale(country[-training, measures1], center = attr(train, \"scaled:center\"), scale = attr(train, \"scaled:scale\"))\n",
    "\n",
    "\n",
    "#Rescale testing data according to how we scaled training data\n",
    "count.som3 <- xyf(data=train, classvec2classmat(country$CLIFS_cat[training]), grid = somgrid(10, 10, \"hexagonal\"), rlen = 1000)\n",
    "summary(count.som3)\n",
    "#xweight - allows to weight the set of training variables versus the prediction variable in the training algorithm\n",
    "\n",
    "#accuracy of the prediction:\n",
    "pos.prediction <- predict(count.som3, newdata = test)\n",
    "table(country[-training, \"Pos\"], pos.prediction$prediction)\n",
    "\n",
    "\n",
    "\n",
    "##########Banks\n",
    "##################\n",
    "#data\n",
    "##################\n",
    "mydata <- read.csv(file=\"~/Desktop/Diplomka/variables/mydata.csv\", header=TRUE, sep=\",\", na.strings=c(\"\",\"NA\"))\n",
    "sapply(mydata,function(x) sum(is.na(x)))\n",
    "\n",
    "mydata = na.omit(mydata)\n",
    "\n",
    "measures1 = c(\"UBPRE600\", \"UBPRE595\", \n",
    "              \"UBPRK447\", \"UBPRE591\", \"UBPRE597\", \"UBPRE021\", \"UBPR7316\", \"UBPRE027\", \n",
    "              \"UBPR7408\", \"UBPRE541\", \"UBPR7414\", \"UBPRE023\", \"UBPRE542\", \"UBPRE006\", \n",
    "              \"UBPRE024\", \"UBPR7402\", \"UBPRE025\", \"UBPRE007\", \"UBPRE013\", \"UBPRE003\", \"UBPRE004\", \n",
    "              \"UBPRE005\", \"UBPRE018\", \"UBPRE017\", \"UBPRE016\", \"UBPRE015\", \"UBPRE029\", \n",
    "              \"UBPRE028\", \"UBPRD486\", \"UBPRE633\", \"UBPRD487\", \"UBPRD488\", \"UBPRE630\", \n",
    "              \"UBPRE088\", \"UBPR7400\")\n",
    "\n",
    "risk_category = c(\"X2008.2009\")\n",
    "\n",
    "mydata0 = mydata\n",
    "\n",
    "for (i in 1: length(measures1)){\n",
    "  mydata[,measures1[i]] = as.numeric(as.character(mydata[,measures1[i]]))\n",
    "}\n",
    "\n",
    "#str(mydata[,X_raw])\n",
    "#boxplot(mydata[,measures1[1]], main=measures1[1])\n",
    "# for (i in 1:length(measures1)){\n",
    "#   boxplot(mydata[,measures1[i]],main=measures1[i])\n",
    "# }\n",
    "\n",
    "#outliers\n",
    "#x<-quantile(mydata[,X_raw[2]],c(0.01,0.99), na.rm=TRUE)\n",
    "for (i in measures1){\n",
    "  x<-quantile(mydata[,i],c(0.01,0.98))\n",
    "  mydata = mydata[mydata[,i] >=x[1] & mydata[,i]<=x[2],]\n",
    "}\n",
    "\n",
    "#train/test\n",
    "training <- sample(nrow(mydata), round(dim(mydata)[1]/100*90,0))\n",
    "Xtrain_raw <- mydata[training, measures1]\n",
    "Xtest_raw = mydata[-training, measures1]\n",
    "Ytrain = as.factor(mydata[training, risk_category])\n",
    "Ytest = as.factor(mydata[-training, risk_category])\n",
    "\n",
    "\n",
    "#Scale\n",
    "Xtrain <- scale(Xtrain_raw)\n",
    "Xtest = scale(Xtest_raw, center = attr(Xtrain, \"scaled:center\"), scale = attr(Xtrain, \"scaled:scale\"))\n",
    "\n",
    "trainingdata <- list(measurements = Xtrain,risk_category = Ytrain)\n",
    "testdata <- list(measurements = Xtest, risk_category = Ytest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ################################################################\n",
    "## Situation 0: obtain expected values for training data (all layers,\n",
    "## also if not used in training) on the basis of the position in the map\n",
    "#mygrid = somgrid(30, 30, \"hexagonal\")\n",
    "# som.risk <- supersom(trainingdata, grid = mygrid, rlen=4000)\n",
    "# som.prediction <- predict(som.risk)\n",
    "\n",
    "## ###############################################################\n",
    "## Situation 3: predictions for layers not present in the original\n",
    "## data. Training data need to be provided for those layers.\n",
    "mygrid = somgrid(35, 30, \"hexagonal\")\n",
    "som.risk <- supersom(Xtrain, \n",
    "                     grid = mygrid, \n",
    "                     rlen=100,\n",
    "                     alpha=c(0.05,0.01),\n",
    "                     keep.data = TRUE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors <- function(n, alpha = 1) {rev(heat.colors(n, alpha))}\n",
    "colors0 <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}\n",
    "colors1 <- function(n, alpha = 1) {rev(terrain.colors(n, alpha))}\n",
    "colour1 <- tricolor(som.risk$mygrid)\n",
    "\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='category - crises',shape='straight', palette.name=colors)\n",
    "\n",
    "#plot(som.risk, type=\"counts\", shape = \"straight\", palette.name = colors, main = \"Default SOM Plot\")\n",
    "par(mfrow=c(1,1))\n",
    "\n",
    "\n",
    "#som.risk1 <- supersom(Xtrain, grid = mygrid, rlen=100)\n",
    "som.prediction <- predict(som.risk, newdata = testdata,\n",
    "                          trainingdata = trainingdata)\n",
    "table(mydata[-training, risk_category], som.prediction$predictions[[\"risk_category\"]])\n",
    "\n",
    "\n",
    "#Iteration\n",
    "plot(som.risk, type=\"changes\")\n",
    "\n",
    "\n",
    "#Count\n",
    "par(mfrow = c(1,1))\n",
    "plot(som.risk, type=\"counts\", shape = \"straight\", palette.name = colors1, main = \"Default SOM Plot\")\n",
    "\n",
    "\n",
    "\n",
    "#Neighbour Distance\n",
    "#the “U-Matrix” - visualisation of the distance between each node and its neighbours.Areas of low neighbour distance indicate groups of nodes that are similar. Areas with large distances indicate the nodes are much more dissimilar – and indicate natural boundaries between node clusters. The U-Matrix can be used to identify clusters within the SOM map. \n",
    "plot(som.risk, type=\"dist.neighbours\", palette.name = colors, shape='straight', main=\"The Unified Distance Matrix\")\n",
    "\n",
    "\n",
    "#because low dimension - we can use Codes / Weight vectors\n",
    "#The node weight vectors, or “codes”, are made up of normalised values of the original variables used to generate the SOM. Each node’s weight vector is representative / similar of the samples mapped to that node. By visualising the weight vectors across the map, we can see patterns in the distribution of samples and variables. The default visualisation of the weight vectors is a “fan diagram”, where individual fan representations of the magnitude of each variable in the weight vector is shown for each node. Other represenations are available, see the kohonen plot documentation for details. \n",
    "plot(som.risk, type=\"codes\", shape='straight')\n",
    "\n",
    "\n",
    "#Heatmap\n",
    "#variables \n",
    "#- shows distribution of variables across map\n",
    "#useful when number of parameters<5\n",
    "par(mfrow = c(2, 2),     # 2x2 layout\n",
    "    oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin\n",
    "    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots\n",
    "    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row\n",
    "    xpd = NA) \n",
    "for (i in 1:53) plot(som.risk, type = \"property\", property = som.risk$codes[[1]][,i], main=colnames(som.risk$data[[1]])[i], palette.name=colors)\n",
    "#THOUGH this default visualisation plots the normalised version of the variable of interest. A more intuitive and useful visualisation is of the variable prior to scaling – use the aggregate function to regenerate the variable from the original training set and the SOM node/sample mappings. The result is scaled to the real values of the training variable.\n",
    "#Aside: Heatmaps with empty nodes in your SOM grid\n",
    "#In some cases, your SOM training may result in empty nodes in the SOM map. In this case, you won’t have a way to calculate mean values for these nodes in the “aggregate” line above when working out the unscaled version of the map. With a few additional lines, we can discover what nodes are missing from the som_model$unit.classif and replace these with NA values – this step will prevent empty nodes from distorting your heatmaps.\n",
    "\n",
    "#A: Examine Heatmaps\n",
    "#Color the crises element: 1(low)-4(high)\n",
    "var_unscaled = aggregate(as.numeric(as.character(Ytrain)), by=list(som.risk$unit.classif), FUN=mean, simplify=TRUE)\n",
    "names(var_unscaled) = c(\"Node\", \"Value\")\n",
    "plot(som.risk, type = \"property\", property=var_unscaled$Value, main='category - crises',shape='straight', palette.name=colors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#banks_rssd = {\"Bank of New York Mellon\": 3587146, \"Bank of America\":1073757, \"BB_T Corporation\":1074156, \"Capital One\":2277860, \"Citigroup\":1951350, \"Fifth Third Bancorp\":1070345, \"Goldman Sachs\":2380443, \"JP Morgan Chase\":1039502, \"KeyCorp\":1034806, \"Morgan Stanley\":2162966, \"PNC, State Street\":1069778, \"Sun Trust\":1131787, \"Regions Financial_2004\":1078332,\"Regions Financial_2016\":3242838, \"U.S. Bancorp\":1119794, \"Wells Fargo\":1120754}\n",
    "#banks_reuters = {\"Bank of New York Mellon\": ['BNY Mellon', 'Bank of New York Mellon', 'BK.N'], \"Bank of America\":['BAC.N', 'Bank of America', 'Bank of America\\'s'], \"BB_T Corporation\":['BB&T Corp', 'BB&T'], \"Capital One\":['COF.N', 'Capital One'], \"Citigroup\":['Citigroup', 'CIT2.L'], \"Fifth Third Bancorp\":['Fifth Third', 'FITB.OQ'], \"Goldman Sachs\":['Goldman', 'GS.N'], \"JP Morgan Chase\":['JP Morgan'], \"KeyCorp\":['KEY.N', 'KeyCorp'], \"Morgan Stanley\":['MS.N', 'Morgan Stanley'], \"PNC, State Street\":['PNC'], \"Sun Trust\":['Suntrust', 'SunTrust', 'STI.N'],\"Regions Financial\":['Regions', 'Regions Financial', 'RF.N'], \"U.S. Bancorp\":['Bancorp', 'USB.N'], \"Wells Fargo\":['Wells Fargo', 'WFC.N']}\n",
    "\n",
    "#c(\"Bank of America\",'Bank of New York Mellon', \"BB_T Corporation\", 'Capital One',\"Fifth Third Bancorp\", \"Goldman Sachs\", \"Morgan Stanley\",\"PNC, State Street\")\n",
    "banks = c('1443266','541101', '852320', '112837', '723112','2182786','1456501', '817824') \n",
    "\n",
    "for (i in banks){\n",
    "  print(sum(country_sc[,'ID.RSSD']==i))\n",
    "}\n",
    "\n",
    "\n",
    "#2010Q3\n",
    "year=2016\n",
    "quarter=\"Q1\"\n",
    "\n",
    "country_sc = mydata0\n",
    "country_sc[,'ID.RSSD'] = as.character(country_sc[,'ID.RSSD'])\n",
    "head(country_sc)\n",
    "filter1 = country_sc[which(country_sc[,'ID.RSSD'] %in% banks),]\n",
    "#add names of banks\n",
    "filter1[,'company'] = 0\n",
    "filter1[which(filter1$ID.RSSD=='1443266'),'company'] = \"Bank of America\"\n",
    "filter1[which(filter1$ID.RSSD=='541101'),'company'] = 'Bank of New York Mellon'\n",
    "filter1[which(filter1$ID.RSSD=='852320'),'company'] = \"BB_T Corporation\"\n",
    "filter1[which(filter1$ID.RSSD=='112837'),'company'] = 'Capital One'\n",
    "filter1[which(filter1$ID.RSSD=='723112'),'company'] = \"Fifth Third Bancorp\"\n",
    "filter1[which(filter1$ID.RSSD=='2182786'),'company'] = \"Goldman Sachs\"\n",
    "filter1[which(filter1$ID.RSSD=='1456501'),'company'] = \"Morgan Stanley\"\n",
    "filter1[which(filter1$ID.RSSD=='817824'),'company'] = \"PNC, State Street\"\n",
    "\n",
    "\n",
    "filter = filter1[which(filter1[,'Quarter']==quarter & filter1[,'year']==year),]\n",
    "\n",
    "xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "som.risk$data = xx$predictions\n",
    "som.risk$unit.classif = xx$unit.classif\n",
    "plot(som.risk, type=\"mapping\",\n",
    "     labels = filter[,'company'],\n",
    "     shape='straight',main='2006')\n",
    "\n",
    "\n",
    "\n",
    "bank=banks[1]\n",
    "filter = filter1[which(filter1[,'ID.RSSD']==bank & filter1[,'Quarter']==quarter),]\n",
    "xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "som.risk$data = xx$predictions\n",
    "som.risk$unit.classif = xx$unit.classif\n",
    "plot(som.risk, type=\"mapping\",\n",
    "     labels = filter[,'year'],\n",
    "     shape='straight',main='2006')\n",
    "\n",
    "filter = country_sc[which(country_sc['year']==2006 & country_sc['Quarter']=='Q3'),]\n",
    "xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "xx = predict(som.risk, as.matrix(filter[,measures1]))\n",
    "som.risk$data = xx$predictions\n",
    "som.risk$unit.classif = xx$unit.classif\n",
    "\n",
    "plot(som.risk, type=\"mapping\",\n",
    "     labels = filter[,'location'],\n",
    "     shape='straight',main='2006')\n",
    "\n",
    "\n",
    "\n",
    "#Hierarchical clustering\n",
    "mydata <- som.risk$codes[[1]]\n",
    "wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var)) \n",
    "for (i in 1:35) {\n",
    "  wss[i] <- sum(kmeans(mydata, centers=i)$withinss)\n",
    "}\n",
    "plot(wss)\n",
    "## use hierarchical clustering to cluster the codebook vectors\n",
    "som_cluster <- cutree(hclust(dist(som.risk$codes[[1]])), 4)\n",
    "# plot these results:\n",
    "plot(som.risk,type=\"code\", bgcol = pretty_palette[som_cluster], main = \"Clusters\", shape='straight') \n",
    "add.cluster.boundaries(som.risk, som_cluster)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
